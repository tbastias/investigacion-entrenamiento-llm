# ğŸ§± Capas del entrenamiento de un modelo

Entrenar un modelo suele involucrar varias capas:
- CÃ¡lculo numÃ©rico (tensores, operaciones)
- Uso de GPU (aceleraciÃ³n)
- Framework de deep learning
- Modelos preentrenados
- Fine-tuning optimizado

Cada librerÃ­a vive en una de esas capas.

## ğŸ”¢ NumPy

**Â¿QuÃ© es?**  
LibrerÃ­a base para cÃ¡lculo numÃ©rico en Python.

**Â¿Para quÃ© se usa?**  
- Operaciones matemÃ¡ticas.
- Arreglos y matrices.

**Importante saber:**

âŒ No se usa para entrenar modelos grandes  
âœ”ï¸ Es la base conceptual de todo

## ğŸ”¥ PyTorch (torch)

**Â¿QuÃ© es?**  
El framework de deep learning mÃ¡s usado hoy.

**Â¿Para quÃ© sirve?**
- Definir redes neuronales
- Entrenar modelos
- Manejar tensores
- Backpropagation automÃ¡tico

Ejemplo mental: _â€œCon PyTorch escribo el modelo y el entrenamientoâ€_

âœ”ï¸ Muy flexible  
âœ”ï¸ Ideal para investigaciÃ³n y producciÃ³n  
âœ”ï¸ Base de casi todo lo moderno

## ğŸš€ CUDA

**Â¿QuÃ© es?**  
TecnologÃ­a de NVIDIA para usar la GPU.

**Â¿Para quÃ© sirve?**
- Acelerar cÃ¡lculos
- Entrenar modelos mucho mÃ¡s rÃ¡pido

**Importante:**

âŒ No es Python  
âŒ No la usÃ¡s directamente  
âœ”ï¸ PyTorch la usa por debajo  

Ejemplo mental: _â€œCUDA es el motor, PyTorch es el volanteâ€_

## âš™ï¸ cuDNN

**Â¿QuÃ© es?**  
LibrerÃ­a de NVIDIA optimizada para redes neuronales.

**Â¿Para quÃ© sirve?**  
Operaciones rÃ¡pidas para deep learning.

âœ”ï¸ Totalmente invisible para el usuario  
âœ”ï¸ Se activa sola si tenÃ©s GPU compatible

## ğŸ¤— Transformers (Hugging Face)

**Â¿QuÃ© es?**  
LibrerÃ­a con modelos preentrenados listos para usar.

**Â¿Para quÃ© sirve?**
- Usar LLaMA, BERT, GPT-like, etc.
- Fine-tuning
- Inferencia

Ejemplo mental: _â€œNo entreno desde cero, empiezo con un modelo ya inteligenteâ€_

âœ”ï¸ Ahorra meses de entrenamiento  
âœ”ï¸ Ideal para principiantes

## ğŸ“¦ Datasets (Hugging Face)

**Â¿QuÃ© es?**  
LibrerÃ­a para manejar datasets grandes.

**Â¿Para quÃ© sirve?**
- Descargar datasets
- Procesarlos
- Stream de datos

âœ”ï¸ Muy usada junto con transformers

## âš¡ Accelerate

**Â¿QuÃ© es?**  
Herramienta para escalar el entrenamiento.

**Â¿Para quÃ© sirve?**
- Multi-GPU
- CPU + GPU
- ConfiguraciÃ³n simple

Ejemplo mental: _â€œQuiero entrenar sin preocuparme por la infraestructuraâ€_

## ğŸ§  PEFT

**Â¿QuÃ© es?**  
Parameter Efficient Fine-Tuning.

**Â¿Para quÃ© sirve?**
- Fine-tuning sin entrenar todo el modelo
- TÃ©cnicas como LoRA

âœ”ï¸ Consume menos GPU  
âœ”ï¸ Ideal para LLMs grandes

## ğŸ¦¥ Unsloth

**Â¿QuÃ© es?**  
Framework optimizado para fine-tuning rÃ¡pido de LLMs.

**Â¿Para quÃ© sirve?**
- Fine-tuning de LLaMA, Mistral, etc.
- Mucho menos VRAM
- MÃ¡s velocidad

Ejemplo mental: _â€œFine-tuning rÃ¡pido en una GPU chicaâ€_

âœ”ï¸ Usa PyTorch + Transformers por debajo  
âœ”ï¸ Muy popular para setups locales

## ğŸ§© BitsAndBytes

**Â¿QuÃ© es?**  
LibrerÃ­a para cuantizaciÃ³n.

**Â¿Para quÃ© sirve?**
- Modelos en 4-bit / 8-bit
- Menor uso de memoria

âœ”ï¸ Clave para GPUs con poca VRAM

## ğŸ“Š Trainer (Transformers)

**Â¿QuÃ© es?**  
Clase que simplifica el entrenamiento.

**Â¿Para quÃ© sirve?**
- Entrenar sin escribir mucho cÃ³digo
- Manejar epochs, logs, evaluaciÃ³n

âœ”ï¸ Ideal para empezar  
âŒ Menos control fino

---

# ğŸ§  Conceptos base del entrenamiento

## Dataset

Conjunto de datos usados para entrenar el modelo.

En LLMs suele ser texto (contenido en archivos `.json` o `.csv`), que luego se transforma en tokens.

## Token

Unidad mÃ­nima de texto que entiende el modelo.

No son palabras exactas: pueden ser partes de palabras, sÃ­mbolos o signos.

## Tokenizer

Herramienta que convierte texto â†’ tokens (y viceversa).

_Ejemplo: SentencePiece, BPE, LLaMA tokenizer._

## Embedding

RepresentaciÃ³n numÃ©rica (vector) de cada token.

Permite que el modelo â€œentiendaâ€ relaciones semÃ¡nticas.

_Ejemplo: Word2Vec, LLaMA / Transformer embeddings._

---

# ğŸ“‰ Durante el entrenamiento

## Loss

FunciÃ³n que mide quÃ© tan mal estÃ¡ prediciendo el modelo.

Cuanto mÃ¡s baja la loss â†’ mejor aprende.

_Ejemplo simple: â€œÂ¿QuÃ© tan lejos estÃ¡ la respuesta del modelo de la respuesta correcta?â€_

## Backward Pass (Backpropagation)

Proceso donde el modelo:
- Calcula el error (loss)
- Propaga ese error hacia atrÃ¡s
- Ajusta los pesos

Todo esto lo maneja **PyTorch** (autograd).

## Optimizer

Algoritmo que usa los gradientes para actualizar los pesos.

_Ejemplos:_ Adam, AdamW, SGD

## Learning Rate

TamaÃ±o de los pasos al ajustar los pesos.

- Muy alto â†’ no converge
- Muy bajo â†’ entrena lento

## Epoch / Batch / Step

- **Epoch:** una pasada completa al dataset  
- **Batch:** subconjunto de datos  
- **Step:** una actualizaciÃ³n del modelo

---

# ğŸ—£ï¸ Inferencia y generaciÃ³n de texto (LLMs)
Estos conceptos **no afectan el entrenamiento**, sino **cÃ³mo responde el modelo** una vez entrenado.

## attention_mask

Indica quÃ© tokens debe atender el modelo (`1`) y cuÃ¡les ignorar (`0`).

Se usa principalmente para ignorar tokens de padding.

## pad_token_id

ID del token usado como relleno (`<PAD>`).

Permite que todas las secuencias tengan la misma longitud.

Normalmente se ignora usando `attention_mask`.

## eos_token_id

Token que indica el final de una secuencia.

Cuando el modelo lo genera, **la respuesta se detiene**.

## apply_chat_template

FunciÃ³n que convierte mensajes tipo chat (`system`, `user`, `assistant`) al formato exacto que el modelo espera.

Cada modelo tiene su propio template.

### LLaMA 3

- Roles explÃ­citos (`system`, `user`, `assistant`)
- Orden estricto
- Muy sensible al formato
- Usa tokens especiales propios de Meta

**Formato conceptual:**

```
<|begin_of_text|>
<|system|>
Eres un asistente Ãºtil
<|user|>
Hola
<|assistant|>
```

### Mistral 

- Usa bloques tipo instruct
- El mensaje del usuario va dentro de `[INST] ... [/INST]`
- System prompt opcional
- MÃ¡s tolerante a errores

**Formato conceptual:**
```
<s>[INST] Hola [/INST]
```

Con System Prompt:

```text
<s>[INST] <<SYS>>
Eres un asistente Ãºtil
<</SYS>>
Hola
[/INST]
```

### Qwen / Qwen2 / Qwen2.5stilo ChatML

- Estilo ChatML
- Roles claramente delimitados
- Muy consistente para chat largo
- Menos tolerante que Mistral, mÃ¡s que LLaMA

```text
[BOS]
<role=system>
  instrucciones del sistema
</role>

<role=user>
  mensaje del usuario
</role>

<role=assistant>
  respuesta del asistente
</role>

<role=user>
  siguiente mensaje del usuario
</role>

<role=assistant>
  ...
</role>
```

### Gemma (Google)

- Estilo instruct simple
- No tan orientado a chat multi-turno
- MÃ¡s cercano a â€œprompt â†’ respuestaâ€
- Menos tokens de rol explÃ­citos

**Formato conceptual:**

```text
[BOS]
INSTRUCCIÃ“N:QuÃ© tiene que hacer el modelo

CONTEXTO (opcional):
  InformaciÃ³n adicional

RESPUESTA:


â—``` U

sar el template incorrecto produce respuestas errÃ¡ticas.

---

# ğŸ›ï¸ Control de generaciÃ³n (sampling)

Estos parÃ¡metros definen **quÃ© tan creativa o determinista** es la respuesta.

## do_sample

Indica si el modelo debe usar muestreo probabilÃ­stico.

- `False` â†’ siempre elige el token mÃ¡s probable
- `True` â†’ elige tokens segÃºn probabilidades

## Multinomial Sampling

MÃ©todo de muestreo usado cuando `do_sample=True`.

El token se elige mediante un sorteo ponderado por probabilidad.

## temperature

Controla la aleatoriedad de las probabilidades.

- Baja (`0.2`) â†’ respuestas mÃ¡s seguras
- Alta (`1.0+`) â†’ mÃ¡s creatividad

## top_k

Limita la elecciÃ³n a los `k` tokens mÃ¡s probables.

Reduce ruido y tokens raros.

## top_p (nucleus sampling)

Elige tokens cuya probabilidad acumulada sea menor a `p`.

MÃ¡s dinÃ¡mico que `top_k`.

---

# âš™ï¸ ImplementaciÃ³n de atenciÃ³n

## attn_implementation="eager"

Usa la implementaciÃ³n clÃ¡sica de atenciÃ³n.

âœ”ï¸ MÃ¡s compatible  
âœ”ï¸ Ideal para debugging o CPU  
âŒ MÃ¡s lenta que flash attention

Otras opciones:
- `sdpa`
- `flash_attention_2`

---

# ğŸ§  LoRA (Low-Rank Adaptation)

MÃ©todo para adaptar modelos grandes congelando sus pesos y entrenando matrices pequeÃ±as adicionales.

âœ”ï¸ Menor consumo de memoria  
âœ”ï¸ Ideal para LLMs grandes  
âœ”ï¸ Base de PEFT
